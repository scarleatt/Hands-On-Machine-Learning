{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / Exploding gradients\n",
    "\n",
    "solving:\n",
    "+ ReLu\n",
    "+ Leaky ReLu\n",
    "+ ELu\n",
    "+ Batch Normalization\n",
    "+ Gradient Clipping during backpropagation so that they never exceed some threshold (mostly useful for RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of simgoid function: $f(x)=f(x) * (1 - f(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def de_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - (sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h/sTvqf3XQs6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, de_sigmoid(z))\n",
    "plt.plot([-5, 5], [0.25, 0.25], 'k.')\n",
    "plt.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络中参数更新需要用到 Derivative(simgoid), 从图中可以看到最大值为0.25，随着层数的增加，乘积会越来越小，会发生梯度消失问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z < 0, 0, z)\n",
    "\n",
    "def de_relu(z):\n",
    "    return np.where(z < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.subplot(121); plt.plot(z, relu(z)); plt.title(\"ReLu\")\n",
    "plt.subplot(122); plt.plot(z, de_relu(z)); plt.title(\"Derivative of ReLU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU优势：\n",
    "+ 解决了梯度消失问题；\n",
    "+ 加快训练速度.\n",
    "\n",
    "缺点：\n",
    "+ $z < 0$时值为0，梯度还是会消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                         kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function is not perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05))\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.title(r\"Leaky ReLU activation function ($\\alpha=0.05$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load mnist datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy 0.86 Validation accuracy 0.9044\n",
      "1 Batch accuracy 0.92 Validation accuracy 0.9226\n",
      "2 Batch accuracy 0.94 Validation accuracy 0.9324\n",
      "3 Batch accuracy 0.92 Validation accuracy 0.9392\n",
      "4 Batch accuracy 0.94 Validation accuracy 0.9464\n",
      "5 Batch accuracy 0.94 Validation accuracy 0.9496\n",
      "6 Batch accuracy 1.0 Validation accuracy 0.956\n",
      "7 Batch accuracy 0.96 Validation accuracy 0.957\n",
      "8 Batch accuracy 0.96 Validation accuracy 0.9606\n",
      "9 Batch accuracy 0.92 Validation accuracy 0.961\n",
      "10 Batch accuracy 0.92 Validation accuracy 0.9654\n",
      "11 Batch accuracy 0.98 Validation accuracy 0.9668\n",
      "12 Batch accuracy 0.98 Validation accuracy 0.968\n",
      "13 Batch accuracy 0.96 Validation accuracy 0.9688\n",
      "14 Batch accuracy 1.0 Validation accuracy 0.9704\n",
      "15 Batch accuracy 0.94 Validation accuracy 0.971\n",
      "16 Batch accuracy 1.0 Validation accuracy 0.9726\n",
      "17 Batch accuracy 1.0 Validation accuracy 0.973\n",
      "18 Batch accuracy 0.98 Validation accuracy 0.9742\n",
      "19 Batch accuracy 0.96 Validation accuracy 0.9748\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy\", acc_batch, \"Validation accuracy\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.where(condition, x, y)\n",
    "\n",
    "if (condition):\n",
    "    return x;\n",
    "else:\n",
    "    return y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(x, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPW9//HXJyEsYQtrRAICIgjK\nJgEV2xq3Ki6t+454q6JYa72VWrX6u/ZqtfVqa9XrQrGX3X2pIta6xQ1Eksi+ryHsIQwhezLz/f0x\nIwQSTCCTnFnez8djHmTmO+d7PvPlzDsn53znjDnnEBGR2JHgdQEiIhJeCnYRkRijYBcRiTEKdhGR\nGKNgFxGJMQp2EZEYo2AXEYkxCnYRkRijYJdGZWZLzSwjmvo2s/5m9p2Z7TWzO8Pd/w+stzHH6jEz\nu6sx+j7MOr41sxO8riPWKdijiJltMLNSMyuqdnu2WtvZh1jOmVnfgx57yMymN0J9B9TgnDvBOZcZ\nyX3X4h4g0znX1jn3dCP0D9R8TY31esysC3AD8GK4+z7E+u4wsywzKzezyQc1PwH8d1PUEc8U7NHn\nIudcm2q3O7wuKAYdAyz1uogwuhGY7ZwrbaL1bQEeAf5RS9u7wBlm1q2JaolLCnapwczuNbO1oUMR\ny8zskoPae5jZW2a208x2mdmzZjYN6Am8F/pL4p7QczeY2dmhPt84qJ+/mdnTda2zrr5DPw8ws0wz\n84UOafys2vIbzGyCmS0ysz1m9qqZtTzEa/8UOAN4NrSufgf/xWNmk83skfr0XdtYHeo1HbwHH67X\nBIwGPj/odTYzswdD/ewys2tDNfz+EH3Um3PuLefcO8CuWtrKgGzgpw1djxyagl1qsxb4MdAe+AMw\n/fs9LDNLBGYBG4FeQHfgFefcGCCX/X9RPH5Qny8D55tZu2r9XAnMrGuddfVtZknAe8C/ga7Ar4AZ\nZta/2tOuBM4DegODCe7F1uCcOxP4ErgjtK5V9RivWvs+1Fg19WsCBgErD3rsEeB0YEhouQdD/x5w\n6MnMZoV+sdR2m1XHuBzK8tB6pZEo2KPPOwe9uW4J9wqcc68757Y45wLOuVeB1cDIUPNI4Gjgt865\nYudcmXPuq3r0uRHIAS4OPXQmUOKc+6Ye66zLKUAb4E/OuQrn3KcEA/Waas95OtR/AcHAHFrPvuvj\nUH0f0ViFhPM1pQB7v78T+uV6FzDOObcHmAccD0x3zu2tvqBz7kLnXMohbhfW87UcbG+oJmkkCvbo\nc/FBb66/12MZP5B00GNJQGVtTzazG8xswfe/PIATgc6h5h7ARudc1RHUPpP9wXQt+/fW61pnXY4G\nNjnnAtUe20hwD/l726r9XEIwNMPlUH03ZKzC+Zp2A22r3T8TWOWcWxe63xzYAzxzBHUeibaAr4nW\nFZcU7PEhl+ChgOp6EwyKA5jZMcDfgTuATs65FGAJYKGnbAJ6mlmzWtZT18X9XwcyzCwNuIRQsNdj\nnXX1vQXoYWbVt+eewOY66qmvEiC52v2j6rncD40VNN1rWgT0q3b/6FD/3xsHbD54bx3AzD6wA2dh\nVb99cAS1AAwAFh7hslIPCvbYkmRmLavdvg+UV4EHzCzNzBJCJ+guAt6opY/WBANnJ4CZ/QfBvefv\nfQtsBf5kZq1D6zkt1LYd6HOo4pxzO4FM4P+A9c655fVcZ119zwOKgXvMLMmCc8EvInQ8OwwWANea\nWaKZnUfw2HR9/NBYQdO9ptkH1ZwHDDWzbmZ2MjAG6GpmzQ9e0Dk3+qBZWNVvo2tbWejEbEsgEUis\nvi2aWQtgOPDREbwOqScFe/R5zw7ca3q7WttsoLTa7aHQ4/8NzAG+Ivhn+ePAdc65JQd37pxbBjwJ\nzCUYPIOAr6u1+wkGTF+CfwnkAVeFmh8j+AvEZ2YTDlH/TOBsqh2GqWuddfXtnKsAfkZw9kc+8Bxw\ng3NuxSFqOFy/JviafcB1wDv1WaiOsYKme01TCZ64bhW6/y+CJ2WXEzypfSnBX16fHkHftXmA4PZ3\nL3B96OcHQm0/I/gZgS2HWFbCwPTVeCKxz8weBXY4557yuI55wE217VRI+CjYRURijA7FiIjEGAW7\niEiMUbCLiMSYQ82vbVSdO3d2vXr18mLV+xQXF9O6dWtPa4gUGouglStX4vf7GThwoNelRIRI3i7y\ndpeyu6SCHh2SSUk++LN34RcpY5GdnZ3vnOtS1/M8CfZevXqRlZXlxar3yczMJCMjw9MaIoXGIigj\nIwOfz+f5thkpInW7mPTlOh55fzl/PKMvE87tX/cCYRApY2FmNT5UWBsdihGRqPHZyh08Ons5556Q\nym/O6Vf3AnFKwS4iUWH19r3cOfM7jj+qHX+9aigJCVb3QnFKwS4iEa+guIKbpmTRIimRSWPTSW7u\nyVHkqNHgYA9dB+JbM1sY+jKAP4SjMBERgIqqAOOnZ7OtsIyJNwzn6JRWdS8U58Lxa68cONM5VxT6\ncoCvzOyD76+zLSJypJxz/Ne7S5i3voC/XjWEk3p28LqkqNDgYHfBaxIUhe4mhW66ToGINNjkORt4\n+dtNjM84lkuGpXldTtQIy4Gq0FeAZRO8it3/Oufm1fKccQSv+0xqaiqZmZnhWPURKyoq8ryGSKGx\nCPL5fPj9fo1FiNfbxeKdVfwlu5xhXRMZ0WIrmZnb6l6okXg9FocrrBcBM7MU4G3gVz909bb09HTn\n9VzhSJmXGgk0FkHfz2NfsGCB16VEBC+3izU7irjkua/pntKKN8ePonULb0+WRsp7xMyynXPpdT0v\nrLNinHM+gl+kcF44+xWR+OErqeDmKfNp0SyBSWPTPQ/1aBSOWTFdQnvqhC7kfzYQri84EJE4UukP\ncPuMHLb4ynhxzHDSOiTXvZDUEI5fhd2AKaHj7AnAa865WWHoV0TiiHOOh95dypy1u3jyiiEMP6aj\n1yVFrXDMilkEDAtDLSISx6Z9s5EZ83K59fQ+XDZcM2AaQp88FRHPfbl6J394bxlnHd+Ve8493uty\nop6CXUQ8tW5nEb+ckUPfLm342zXDSNQ1YBpMwS4intlTUsnNU7JolhicAdNGM2DCQsEuIp6o9Af4\n5cwcNu0u4cUxw+nRUTNgwkW/HkXEEw/PWsZXa/J5/PLBjOilGTDhpD12EWly077ZyNS5G7nlx725\nMr2H1+XEHAW7iDSpOWvyeejdpZzRvwv3jh7gdTkxScEuIk1mfX4x42fk0Kdza57WDJhGo2AXkSax\np7SSm6bMJ8HgpbEjaNsyyeuSYpZOnopIo6vyB7hjZg65u0qYfvPJ9OykGTCNScEuIo3ukfeX8+Xq\nfP506SBO6dPJ63Jing7FiEijmjkvl8lzNvCL03pz9cieXpcTFxTsItJo5q7dxf/75xJO79eF+8/X\nNWCaioJdRBrFxl3FjJ+RzTGdknnm2mE0S1TcNBWNtIiEXWFZJTdNCX795UtjR9BOM2CalIJdRMLK\nH3Dc+fJ3bMgv5rnrTqJX59ZelxR3NCtGRMLq0dnLyVy5kz9eciKjju3sdTlxSXvsIhI2r87P5aWv\n1nPjqF5cd/IxXpcTtxTsIhIW89bt4oF3lvDj4zrzwAW6BoyXFOwi0mCbCkq4bXo2PTom8+y1J2kG\njMc0+iLSIHvLgteACbjgDJj2rTQDxms6eSoiR8wfcPz6lQWs3VnMlP8YSW/NgIkI2mMXkSP253+t\n4NMVO3joooH86DjNgIkUCnYROSKvZ21i4hfrGHPKMYw5tZfX5Ug1CnYROWzzNxRw/9uLOa1vJ/7f\nRQO9LkcOomAXkcOyqaCEW6dlk9YhmeeuHU6SZsBEHP2PiEi9FZVXccvULKr8ASaNTad9smbARCLN\nihGRegkEHHe9soDVO4qY/B8jOLZLG69LkkPQHruI1MvjH67k4+XbefCCAfz4uC5elyM/QMEuInV6\nMzuPFz5fy7Un92TsqF5elyN1ULCLyA/K3ljAfW8t5tQ+nfjDz07AzLwuSeqgYBeRQ8rbHZwB0y2l\nJc9dd5JmwESJBv8vmVkPM/vMzJab2VIz+3U4ChMRb5VVOW6ekkV5ZYCXxqbToXVzr0uSegrHrJgq\n4G7nXI6ZtQWyzewj59yyMPQtIh4IBBwTF5Wzaqeff9w4gr5d23pdkhyGBu+xO+e2OudyQj/vBZYD\n3Rvar4h458mPVpKzw88DFwwko39Xr8uRwxTWeexm1gsYBsyrpW0cMA4gNTWVzMzMcK76sBUVFXle\nQ6TQWAT5fD78fn/cj8WcLVVMXFTOqFRH78oNZGZu9Lokz0XbeyRswW5mbYA3gbucc4UHtzvnJgIT\nAdLT011GRka4Vn1EMjMz8bqGSKGxCEpJScHn88X1WHyXu5vJH3/Dyb078ovjyjjjjDO8LikiRNt7\nJCynuM0siWCoz3DOvRWOPkWkaW3xlXLL1GyOateS568fTrMETWuMVg3eY7fgpNaXgOXOub80vCQR\naWolFVXcPCWLsko/M285mY6aARPVwrHHfhowBjjTzBaEbueHoV8RaQKBgOPu1xayYlshz1wzjH6p\nmgET7Rq8x+6c+wrQ32wiUeqpj1fxwZJtPHDBAM44XjNgYoE+RiYSx95duIWnP13Dlelp3PSj3l6X\nI2GiYBeJUws2+fjt6wsZ0asDD198oq4BE0MU7CJxaNueMsZNzaJL2xa8cP1wWjRL9LokCSMFu0ic\nKa3wc8vULIrLq3hp7Ag6tWnhdUkSZvoGJZE4Egg4Jry+kCVb9jDphnT6H6UZMLFIe+wiceTpT1fz\n/uKt3Hve8Zw1INXrcqSRKNhF4sT7i7by1MerueykNMb9pI/X5UgjUrCLxIHFeXu4+/UFDD+mA49e\nqhkwsU7BLhLjtheWcfPU+XRq3YIXx2gGTDzQyVORGFZW6Wfc1Cz2llXx5vhRdNYMmLigYBeJUc45\nfvvGIhZt3sOL1w9nQLd2XpckTUSHYkRi1DOfruG9hVv47bn9+ekJR3ldjjQhBbtIDPpg8Vb+8tEq\nLh3WnfGnH+t1OdLEFOwiMWbJ5j385rWFDOuZwqOXDtIMmDikYBeJITsKy7hlahYdkpN4ccxwWiZp\nBkw80slTkRhRVuln3LRsfCWVvDH+VLq2bel1SeIRBbtIDHDO8bs3F7Fgk48Xrj+JE45u73VJ4iEd\nihGJAc9lruWfC7Yw4af9OO/Ebl6XIx5TsItEuX8t2cb/fLiSnw89ml+e0dfrciQCKNhFotjSLXv4\nz1cXMKRHCn++bLBmwAigYBeJWjv3lnPLlCzat0ri75oBI9Xo5KlIFCqr9HPrtCwKSip447ZRdG2n\nGTCyn4JdJMo457j/rcXk5Pp47rqTOLG7ZsDIgXQoRiTKvPD5Ot76bjP/eXY/zh+kGTBSk4JdJIp8\ntGw7j3+4ggsHd+POszQDRmqnYBeJEsu3FvLrV75jUPf2PHHFEM2AkUNSsItEgfyicm6ekkXbls34\n+w3pmgEjP0gnT0UiXHmVn9umZZNfVM7rt51KqmbASB0U7CIRzDnH799eQtbG3TxzzTAGp6V4XZJE\nAR2KEYlgf/9yHW9k53HnWcdx0ZCjvS5HooSCXSRCfbJ8O499sILzBx3FXWcd53U5EkXCEuxm9g8z\n22FmS8LRn0i8W7ltL3e+/B0nHN2OJ68YSkKCZsBI/YVrj30ycF6Y+hKJa7uKyrlpynxatwjOgGnV\nXDNg5PCEJdidc18ABeHoSySeVVQFGD8jh517y5l4Qzrd2rfyuiSJQk02K8bMxgHjAFJTU8nMzGyq\nVdeqqKjI8xoihcYiyOfz4ff7PRsL5xz/t7SCb/OquG1wC3xrF5C51pNSAG0X1UXbWDRZsDvnJgIT\nAdLT011GRkZTrbpWmZmZeF1DpNBYBKWkpODz+Twbi0lfruOLvOXccUZfJpzb35MaqtN2sV+0jYVm\nxYhEgM9W7uDR2cs594RUfnNOP6/LkSinYBfx2Orte7lz5nccf1Q7/nqVZsBIw4VruuPLwFygv5nl\nmdlN4ehXJNYVFFdw05QsWiQlMmlsOsnN9WFwabiwbEXOuWvC0Y9IPKmoCjB+ejbbCst4ZdwpHJ2i\nGTASHjoUI+IB5xz/9e5S5q0v4PHLBnNSzw5elyQxRMEu4oHJczbw8re53J5xLBcP6+51ORJjFOwi\nTezzVTt5eNYyzhmYyoSfej+tUWKPgl2kCa3ZUcQdM3Pol9qWpzQDRhqJgl2kifhKKrh5ynxaNEtg\n0th0WrfQDBhpHNqyRJpApT/A7TNy2OIr4+VxJ5PWIdnrkiSGKdhFmsAf3lvKnLW7ePKKIQw/pqPX\n5UiM06EYkUY2de4Gpn+Ty62n9+Gy4WlelyNxQMEu0oi+XL2TP7y3jLMHdOWec4/3uhyJEwp2kUay\nbmcRv5yRQ98ubXjq6mEkagaMNBEFu0gj2FNSyc1TsmiWGJwB00YzYKQJKdhFwqzSH+CXM3PYtLuE\nF8cMp0dHzYCRpqXdCJEwe3jWMr5ak8/jlw9mRC/NgJGmpz12kTCa9s1Gps7dyC0/7s2V6T28Lkfi\nlIJdJEy+XpPPQ+8u5czju3Lv6AFelyNxTMEuEgbr84u5fUYOx3Zpzd+uHqoZMOIpBbtIA+0preSm\nKfNJMJh0wwjatkzyuiSJczp5KtIAVf4Ad8zMYVNBCdNvOpmenTQDRrynYBdpgEfeX86Xq/P582WD\nOLlPJ6/LEQF0KEbkiM2Yt5HJczZw0496c9WInl6XI7KPgl3kCMxZm89//XMpGf27cP/5mgEjkUXB\nLnKYNoRmwPTq3Jqnr9E1YCTyKNhFDkNhWSU3T80C4KWx6bTTDBiJQAp2kXqq8gf41czv2JBfzPPX\nDeeYTq29LkmkVpoVI1JPj85eweerdvLYpYM49VjNgJHIpT12kXp45dtc/vH1em4c1YtrRmoGjEQ2\nBbtIHb5Zt4sH3lnCT/p14YELNANGIp+CXeQH5O4qYfz0bHp2SuaZa4bRLFFvGYl82kpFDmFvWfAa\nMAEHL40dQftWmgEj0UHBLlILf8Bx58vfsT6/mOevO4nenTUDRqKHZsWI1OKx2cv5bOVOHrn4REb1\n7ex1OSKHJSx77GZ2npmtNLM1ZnZvOPoU8cqr83OZ9NV6xp56DNefcozX5YgctgYHu5klAv8LjAYG\nAteY2cCG9ivihZIqxwPvLOHHx3XmwQu1GUt0CsehmJHAGufcOgAzewX4ObDsUAusXLmSjIyMMKz6\nyPl8PlJSUjytIVJoLIJyvltAcXkl7V65n/Xd23P2tPi+Boy2i/2ibSzCEezdgU3V7ucBJx/8JDMb\nB4wDSEpKwufzhWHVR87v93teQ6TQWEDAQUl5FQDdkqGocI/HFXlP28V+0TYW4Qj22nZrXI0HnJsI\nTARIT093WVlZYVj1kcvMzPT8r4ZIEe9jUV7l5xeT57P64XGktTGWLV7gdUkRId63i+oiZSzM6vdX\nZDhOnuYBPardTwO2hKFfkUYXCDgmvL6Ir9fsok/n1rROiu/DLxIbwhHs84HjzKy3mTUHrgbeDUO/\nIo3u0dnLeW/hFn533vF0advC63JEwqLBwe6cqwLuAD4ElgOvOeeWNrRfkcb2wudrmfRV8MJet53e\nx+tyRMImLB9Qcs7NBmaHoy+RpjDpy3X86YMVXDTkaB68cGC9j12KRANdUkDizktfreeR95dzwaBu\n/PXKIfpqO4k5CnaJK//39XoenrWM0ScexVNXD9XVGiUm6VoxEhecczz9yRr++vEqzj0hlaevGUaS\nQl1ilIJdYl4g4HjovaVMnbuRy05K40+XDVKoS0xTsEtMK6/yc/drC5m1aCvjftKH+0YfrxOlEvMU\n7BKz8ovKuW1aNlkbd3Pf6OO59fRjvS5JpEko2CUmLdtSyC1Ts9hVXM4z1wzjoiFHe12SSJNRsEvM\nmb14K3e/tpD2rZJ4/dZRDEpr73VJIk1KwS4xo7zKz6PvL2fK3I0M65nCi9cPp2u7ll6XJdLkFOwS\nEzbkF3PHyzks2VzITT/qze/OO57mzTTzReKTgl2imnOO17I28fCs5SQmGH+/IZ1zBqZ6XZaIpxTs\nErW27Snj3rcWkblyJ6f06cgTVwwhrUOy12WJeE7BLlEnEHC8nr2JP76/nAp/gIcuGsgNp/YiQdd8\nEQEU7BJllm0p5IF3FpOT62Nkr478+fLB9O7c2uuyRCKKgl2iwu7iCv72yWqmfbOR9q2SeOKKIVx2\nUnd9ilSkFgp2iWhllX6mzNnAs5+tobi8iqtH9uSec/uTktzc69JEIpaCXSJSpT/Am9l5PPPpGjb7\nSjmjfxfuHT2A/ke19bo0kYinYJeIUlEV4K2cPJ79bA15u0sZktaexy8fzGl9O3tdmkjUULBLRNhd\nXMHMb3OZMmcDO/aWM6RHCg9ffCIZ/broOLrIYVKwi6fW7CjiH1+v562cPMoqA/z4uM48fvlgTleg\nixwxBbs0udIKP/9aupXXs/KYs3YXzZslcOmw7vziR73pl6pj6CINpWCXJuGcIyfXxxvZm5i1cCt7\ny6vo2TGZCT/txzUje9KpTQuvSxSJGQp2aTTOORbm7eGDJVv5YPE2cgtKaJWUyPmDunFFehoje3XU\np0VFGoGCXcKqoipATu5u/r10Ox8u3cZmXynNEozT+nbmjjP7cv6gbrRpoc1OpDHpHSYNtqmghM9X\n7eTzVTuZsyaf4go/zZsl8JPjuvCbc/px9oBU2icneV2mSNxQsMthcc6RW1DCvPUFzF9fwLcbCti4\nqwSA7imtuHhYd07v14VRfTtrz1zEI3rnyQ/aU1LJki17WJS3h8WbfWRv3M32wnIAOiQnMaJXR8ae\n2ovT+3ehT+fWmqIoEgEU7AIE98S3FZaxensRK7YVhoJ8z769cYCeHZMZ2bsTJ/fuyMjeHenbpY1O\nfopEIAV7nCmv8rN5dykbC0pYs72I1Tv2kr26lF999m/2llfte173lFYMTmvPlek9GJzWnkHd2+vC\nWyJRQsEeY6r8AfKLKtiyp5RNBSXk7iohtyB421RQwtbCMpzb//zObZrTuTlcclJ3juvahr5d29L/\nqLZ0bK0QF4lWCvYo4JyjuMLP7uIKCoorKCipYGdhOdsKy9heWMb2wvLQv2XkF5UTcAcun9quBT07\nJnNKn0706JhMz47J9OyUzLFd2tCxdXMyMzPJyDjRmxcnImHXoGA3syuAh4ABwEjnXFY4iopFgYCj\nuKKKvWVVFJVXsbesstrPVRSVBR/bXVJJQUnFvhDfXVLB7uJKKvyBWvvt2Lo5qe1aktquBQO7tSO1\nXQtS27ekW/uW9OyYTFqHZFomJTbxqxURLzV0j30JcCnwYhhqaRLOOaoCjvIqx56SSioDASr9ASqr\nHBX+0M+hW0WVo9IfoKzST1lVgLIKP6WVfsoqg/+WVvoprwxQetDj5ZWBfe3F5cHQLqqoOuAQSG3M\nIKVVEh1aN6djcnN6dExmSFpK8H7rJDokN6dj6+akJDena9sWdG3XghbNFNoicqAGBbtzbjlw2FPc\nFi9dTq9BI8CBI3jrf+o5DD7nKsrLSnjnT78KPu4cjuATjj3tAnqPuoCSwt18+cL9+5cNpWWPUT8n\nddiZFO/azpKZfwy1QSDUR8eTL6HlsSdTvCOXXR8+W6Om9qOuplWvoVRsX0fBJxNrtKf8ZCwt0wZQ\nlrcc3xdTAEgwC94SoN/P76DzMf3Zuy6H1R9OJdGMBIOEBKNZQgJX/ed/0/e4fqz8NpN/vTKJxAQj\n0YxmCUZigvGPyVPo16cXr7/+Gs8//zz5QH619b/xxht07tyZyZMn8/DkyTXqmz17NsnJyTz33HO8\n9tprNdozMzMBeOKJJ5g1a9YBbaWlpcybNw+Ahx9+mE8++eSA9k6dOvHmm28CcN999zF37twD2tPS\n0pg+fToAd911FwsWLDigvV+/fkycGBzTcePGsWrVqgPahw4dylNPPQXA9ddfT15e3gHtp556Ko89\n9hgAl112Gbt27Tqg/ayzzuLBBx8EYPTo0ZSWlh7QfuGFFzJhwgQAMjIyONiVV17J7bffTiAQYM2a\nNTWec+ONN3LjjTeSn5/P5ZdfXmP58ePHc9VVV7Fp0ybGjBlTo/3uu+/moosuYuXKldx666012h94\n4AHOPvtsFixYwF133VWj/dFHH2XUqFHMmTOH+++/v0b7U089xdChQ/n444955JFHarS/+OKL9O/f\nn/fee48nn3yyRvu0adPo0aMHr776Ks8///y+x30+HykpKQdse5PDvO21atWKDz74AIjvba+kpITz\nzz+/Rntd296hNNkxdjMbB4wDsKSW7CgswwAMDGPFZh8FyzbjKssoLK0g1BRaFvL3FNF8RwFVJXuo\nqqwKLce+5zULlJMcKCHBykiy4GELSwi2mcHAjsaxPRPZ27IZn7VKwLkACQkJWKifM3o3o//gFmzf\nmMR73yYElwsta8DVQ1pw4omtWLuiBTMWJ3LwLL/bTjD69g2Q7fczrVXNwyYDm+2gp78lO8q3kVgZ\nnELoD90AFuXMZ1veRpYuXYrP56ux/Ndff0379u1ZsWJFre1ffPEFLVu2ZNWqVbW2f//mWrt2bY32\nxMTEfe3r16+v0R4IBPa15+bm1mhPSkra156Xl1ejfcuWLfvat2zZUqM9Ly9vX/v27dtrtOfm5u5r\n37lzJ4WFhQe0r1+/fl97QUEB5eXlB7SvXbt2X3ttY7Nq1SoyMzPx+Xw452o8Z8WKFWRmZrJnz55a\nl1+6dCmZmZns2LGj1vbFixfTtm3bWscOYOHChTRr1ow1a9bU2p6Tk0NFRQVLliyptT0rKwufz8fC\nhQtrbZ83bx5bt25l8eLFtbbPnTuXtWvX1tj2/H4/Pp+vUbe90tLSqNj2ioqKGnXbKysrq7W9rm3v\nUMzVcXzAzD4Gjqql6ffOuX+GnpMJTKjvMfb09HSXleXt4fjgCcMMT2uIFBqLoIyMDHw+X429vnil\n7WK/SBkLM8t2zqXX9bw699idc2eHpyQREWkKCV4XICIi4dWgYDezS8wsDzgVeN/MPgxPWSIicqQa\nOivmbeDtMNUiIiJhoEMxIiIxRsEuIhJjFOwiIjFGwS4iEmMU7CIiMUbBLiISYxTsIiIxRsEuIhJj\nFOwiIjFGwS4iEmMU7CIiMUY9GqPGAAAEBklEQVTBLiISYxTsIiIxRsEuIhJjFOwiIjFGwS4iEmMU\n7CIiMUbBLiISYxTsIiIxRsEuIhJjFOwiIjFGwS4iEmMU7CIiMUbBLiISYxTsIiIxRsEuIhJjFOwi\nIjFGwS4iEmMU7CIiMUbBLiISYxTsIiIxRsEuIhJjGhTsZvY/ZrbCzBaZ2dtmlhKuwkRE5Mg0dI/9\nI+BE59xgYBVwX8NLEhGRhmhQsDvn/u2cqwrd/QZIa3hJIiLSEM3C2NcvgFcP1Whm44BxAKmpqWRm\nZoZx1YevqKjI8xoihcYiyOfz4ff7NRYh2i72i7axqDPYzexj4Khamn7vnPtn6Dm/B6qAGYfqxzk3\nEZgIkJ6e7jIyMo6k3rDJzMzE6xoihcYiKCUlBZ/Pp7EI0XaxX7SNRZ3B7pw7+4fazWwscCFwlnPO\nhaswERE5Mg06FGNm5wG/A053zpWEpyQREWmIhs6KeRZoC3xkZgvM7IUw1CQiIg3QoD1251zfcBUi\nIiLhoU+eiojEGAW7iEiMMS8mspjZTmBjk6/4QJ2BfI9riBQai/00FvtpLPaLlLE4xjnXpa4neRLs\nkcDMspxz6V7XEQk0FvtpLPbTWOwXbWOhQzEiIjFGwS4iEmPiOdgnel1ABNFY7Kex2E9jsV9UjUXc\nHmMXEYlV8bzHLiISkxTsIiIxRsEOmNkEM3Nm1tnrWryirzkMXtTOzFaa2Rozu9frerxiZj3M7DMz\nW25mS83s117X5DUzSzSz78xslte11EfcB7uZ9QDOAXK9rsVjcf01h2aWCPwvMBoYCFxjZgO9rcoz\nVcDdzrkBwCnAL+N4LL73a2C510XUV9wHO/BX4B4grs8i62sOGQmscc6tc85VAK8AP/e4Jk8457Y6\n53JCP+8lGGjdva3KO2aWBlwATPK6lvqK62A3s58Bm51zC72uJcL8AvjA6yKaWHdgU7X7ecRxmH3P\nzHoBw4B53lbiqacI7vwFvC6kvsL5nacR6Ye+2g+4H/hp01bknXB9zWGMsloei+u/4sysDfAmcJdz\nrtDrerxgZhcCO5xz2WaW4XU99RXzwX6or/Yzs0FAb2ChmUHw0EOOmY10zm1rwhKbjL7m8AflAT2q\n3U8DtnhUi+fMLIlgqM9wzr3ldT0eOg34mZmdD7QE2pnZdOfc9R7X9YP0AaUQM9sApDvnIuEKbk0u\n9DWHfyH4NYc7va6nqZlZM4Injc8CNgPzgWudc0s9LcwDFtzTmQIUOOfu8rqeSBHaY5/gnLvQ61rq\nEtfH2OUAcf01h6ETx3cAHxI8WfhaPIZ6yGnAGODM0LawILTHKlFCe+wiIjFGe+wiIjFGwS4iEmMU\n7CIiMUbBLiISYxTsIiIxRsEuIhJjFOwiIjHm/wPVUBcDZIth3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z))\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four parameters are learned for each batch-normalized layer: $\\gamma$(scale), $\\beta$(offset), $\\mu$(mean), and $\\delta$(standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "+ The vanishing gradients problem was strongly reduced;\n",
    "+ less sensitive to the weights initialization;\n",
    "+ can use much larger learning rates;\n",
    "+ also acts like a regularizerm reducing the need for other regularization techiques(such as dropout).\n",
    "\n",
    "Disadvantages:\n",
    "+ make slower predictions due to the extra computations required at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, \n",
    "                                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_normalization()`:\n",
    "+ `training`:\n",
    "    + `True` (during training), use the current mini-batch's mean and standard deviation;\n",
    "    + `False` (during testing), the running averages that is keeps track of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over again, we can use Python's `partial()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "my_batch_normal_layzer = partial(tf.layers.batch_normalization,\n",
    "                                training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_normal_layzer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2,name=\"hidden2\")\n",
    "bn2 = my_batch_normal_layzer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn= tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_normal_layzer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    my_dense_layer = partial(tf.layers.dense, \n",
    "                             kernel_initializer=he_init)\n",
    "    \n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                                  training=training, \n",
    "                                  momentum=batch_norm_momentum)\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    \n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    \n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.8942\n",
      "1 Validation accuracy 0.9186\n",
      "2 Validation accuracy 0.93\n",
      "3 Validation accuracy 0.9426\n",
      "4 Validation accuracy 0.9468\n",
      "5 Validation accuracy 0.9534\n",
      "6 Validation accuracy 0.9562\n",
      "7 Validation accuracy 0.9594\n",
      "8 Validation accuracy 0.9614\n",
      "9 Validation accuracy 0.965\n",
      "10 Validation accuracy 0.9658\n",
      "11 Validation accuracy 0.9662\n",
      "12 Validation accuracy 0.968\n",
      "13 Validation accuracy 0.9706\n",
      "14 Validation accuracy 0.9706\n",
      "15 Validation accuracy 0.9716\n",
      "16 Validation accuracy 0.972\n",
      "17 Validation accuracy 0.9728\n",
      "18 Validation accuracy 0.9734\n",
      "19 Validation accuracy 0.9748\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], \n",
    "                    feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are using `tf.layers.batch_normalization()` rather than `tf.contrib.layers.batch_norm()` (as in the book), we need to explicitly run the extra update operations needed by batch normalization (`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, you would just have to evaluate the `training_op` during training, TensorFlow would automatically run the update operations as well:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping \n",
    "during backpropagation so that they never exceed some threshold (mostly useful for RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply gradient clipping. For this, we need to get the gradients, use the clip_by_value() function to clip them, then apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "             for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.8768\n",
      "1 Validation accuracy 0.9276\n",
      "2 Validation accuracy 0.9456\n",
      "3 Validation accuracy 0.9546\n",
      "4 Validation accuracy 0.9574\n",
      "5 Validation accuracy 0.9556\n",
      "6 Validation accuracy 0.9672\n",
      "7 Validation accuracy 0.9714\n",
      "8 Validation accuracy 0.9678\n",
      "9 Validation accuracy 0.9722\n",
      "10 Validation accuracy 0.9746\n",
      "11 Validation accuracy 0.9744\n",
      "12 Validation accuracy 0.9702\n",
      "13 Validation accuracy 0.972\n",
      "14 Validation accuracy 0.9766\n",
      "15 Validation accuracy 0.976\n",
      "16 Validation accuracy 0.9758\n",
      "17 Validation accuracy 0.9756\n",
      "18 Validation accuracy 0.9776\n",
      "19 Validation accuracy 0.9744\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/Mean\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"./tf_logs/11/\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/Mean:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9788\n",
      "1 Validation accuracy: 0.9776\n",
      "2 Validation accuracy: 0.976\n",
      "3 Validation accuracy: 0.9786\n",
      "4 Validation accuracy: 0.9766\n",
      "5 Validation accuracy: 0.9788\n",
      "6 Validation accuracy: 0.978\n",
      "7 Validation accuracy: 0.9786\n",
      "8 Validation accuracy: 0.9788\n",
      "9 Validation accuracy: 0.9786\n",
      "10 Validation accuracy: 0.9786\n",
      "11 Validation accuracy: 0.9786\n",
      "12 Validation accuracy: 0.9774\n",
      "13 Validation accuracy: 0.9778\n",
      "14 Validation accuracy: 0.979\n",
      "15 Validation accuracy: 0.978\n",
      "16 Validation accuracy: 0.9776\n",
      "17 Validation accuracy: 0.9794\n",
      "18 Validation accuracy: 0.9782\n",
      "19 Validation accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing a Tensorflow Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy 0.9752\n",
      "1 Validation accuracy 0.9764\n",
      "2 Validation accuracy 0.9768\n",
      "3 Validation accuracy 0.978\n",
      "4 Validation accuracy 0.9766\n",
      "5 Validation accuracy 0.9792\n",
      "6 Validation accuracy 0.9768\n",
      "7 Validation accuracy 0.9758\n",
      "8 Validation accuracy 0.9768\n",
      "9 Validation accuracy 0.9776\n",
      "10 Validation accuracy 0.9762\n",
      "11 Validation accuracy 0.9782\n",
      "12 Validation accuracy 0.9794\n",
      "13 Validation accuracy 0.978\n",
      "14 Validation accuracy 0.9762\n",
      "15 Validation accuracy 0.9776\n",
      "16 Validation accuracy 0.9738\n",
      "17 Validation accuracy 0.9774\n",
      "18 Validation accuracy 0.9796\n",
      "19 Validation accuracy 0.9782\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                              scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)        # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)\n",
    "    \n",
    "    saver_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Models from Other Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, tf.nn.relu, name=\"hidden1\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "    \n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=(n_hidden1))\n",
    "\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    print(hidden1.eval(feed_dict={X: [[10., 11.]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss =  tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                     scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=training_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy 0.9642\n",
      "1 Validation accuracy 0.972\n",
      "2 Validation accuracy 0.9738\n",
      "3 Validation accuracy 0.975\n",
      "4 Validation accuracy 0.9764\n",
      "5 Validation accuracy 0.977\n",
      "6 Validation accuracy 0.9768\n",
      "7 Validation accuracy 0.9776\n",
      "8 Validation accuracy 0.9758\n",
      "9 Validation accuracy 0.978\n",
      "10 Validation accuracy 0.9764\n",
      "11 Validation accuracy 0.9766\n",
      "12 Validation accuracy 0.9772\n",
      "13 Validation accuracy 0.9766\n",
      "14 Validation accuracy 0.977\n",
      "15 Validation accuracy 0.9772\n",
      "16 Validation accuracy 0.976\n",
      "17 Validation accuracy 0.977\n",
      "18 Validation accuracy 0.9764\n",
      "19 Validation accuracy 0.976\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                              scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, \n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, \n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    \n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, \n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, \n",
    "                              name=\"hidden4\") # new !\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy 0.9668\n",
      "1 Validation accuracy 0.9716\n",
      "2 Validation accuracy 0.9742\n",
      "3 Validation accuracy 0.9748\n",
      "4 Validation accuracy 0.9756\n",
      "5 Validation accuracy 0.9772\n",
      "6 Validation accuracy 0.9754\n",
      "7 Validation accuracy 0.9758\n",
      "8 Validation accuracy 0.9766\n",
      "9 Validation accuracy 0.9772\n",
      "10 Validation accuracy 0.976\n",
      "11 Validation accuracy 0.9768\n",
      "12 Validation accuracy 0.9772\n",
      "13 Validation accuracy 0.9756\n",
      "14 Validation accuracy 0.9764\n",
      "15 Validation accuracy 0.977\n",
      "16 Validation accuracy 0.9754\n",
      "17 Validation accuracy 0.9756\n",
      "18 Validation accuracy 0.9764\n",
      "19 Validation accuracy 0.9758\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                              scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cathing the Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, \n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, \n",
    "                              name=\"hidden2\") # reused frozen & cathed\n",
    "    \n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, \n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, \n",
    "                              name=\"hidden4\") # new !\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new !\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                              scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy 0.1234\n",
      "1 Validation accuracy 0.0838\n",
      "2 Validation accuracy 0.0766\n",
      "3 Validation accuracy 0.0862\n",
      "4 Validation accuracy 0.1364\n",
      "5 Validation accuracy 0.1342\n",
      "6 Validation accuracy 0.1192\n",
      "7 Validation accuracy 0.1236\n",
      "8 Validation accuracy 0.1236\n",
      "9 Validation accuracy 0.1178\n",
      "10 Validation accuracy 0.1152\n",
      "11 Validation accuracy 0.114\n",
      "12 Validation accuracy 0.1136\n",
      "13 Validation accuracy 0.1138\n",
      "14 Validation accuracy 0.1134\n",
      "15 Validation accuracy 0.1136\n",
      "16 Validation accuracy 0.1136\n",
      "17 Validation accuracy 0.1134\n",
      "18 Validation accuracy 0.1132\n",
      "19 Validation accuracy 0.1128\n",
      "20 Validation accuracy 0.1128\n",
      "21 Validation accuracy 0.1128\n",
      "22 Validation accuracy 0.1128\n",
      "23 Validation accuracy 0.1128\n",
      "24 Validation accuracy 0.1128\n",
      "25 Validation accuracy 0.1132\n",
      "26 Validation accuracy 0.1134\n",
      "27 Validation accuracy 0.1132\n",
      "28 Validation accuracy 0.1132\n",
      "29 Validation accuracy 0.1128\n",
      "30 Validation accuracy 0.1128\n",
      "31 Validation accuracy 0.1128\n",
      "32 Validation accuracy 0.1128\n",
      "33 Validation accuracy 0.1126\n",
      "34 Validation accuracy 0.1126\n",
      "35 Validation accuracy 0.1128\n",
      "36 Validation accuracy 0.1128\n",
      "37 Validation accuracy 0.1126\n",
      "38 Validation accuracy 0.1126\n",
      "39 Validation accuracy 0.1126\n",
      "40 Validation accuracy 0.1128\n",
      "41 Validation accuracy 0.1126\n",
      "42 Validation accuracy 0.1126\n",
      "43 Validation accuracy 0.1126\n",
      "44 Validation accuracy 0.1126\n",
      "45 Validation accuracy 0.1126\n",
      "46 Validation accuracy 0.1126\n",
      "47 Validation accuracy 0.1126\n",
      "48 Validation accuracy 0.1128\n",
      "49 Validation accuracy 0.1126\n",
      "50 Validation accuracy 0.1128\n",
      "51 Validation accuracy 0.1126\n",
      "52 Validation accuracy 0.1126\n",
      "53 Validation accuracy 0.1128\n",
      "54 Validation accuracy 0.1126\n",
      "55 Validation accuracy 0.1126\n",
      "56 Validation accuracy 0.1126\n",
      "57 Validation accuracy 0.1126\n",
      "58 Validation accuracy 0.1126\n",
      "59 Validation accuracy 0.1126\n",
      "60 Validation accuracy 0.1126\n",
      "61 Validation accuracy 0.1128\n",
      "62 Validation accuracy 0.1126\n",
      "63 Validation accuracy 0.1126\n",
      "64 Validation accuracy 0.1126\n",
      "65 Validation accuracy 0.1126\n",
      "66 Validation accuracy 0.1126\n",
      "67 Validation accuracy 0.1126\n",
      "68 Validation accuracy 0.1126\n",
      "69 Validation accuracy 0.1126\n",
      "70 Validation accuracy 0.1126\n",
      "71 Validation accuracy 0.1126\n",
      "72 Validation accuracy 0.1126\n",
      "73 Validation accuracy 0.1126\n",
      "74 Validation accuracy 0.1126\n",
      "75 Validation accuracy 0.1126\n",
      "76 Validation accuracy 0.1126\n",
      "77 Validation accuracy 0.1126\n",
      "78 Validation accuracy 0.1128\n",
      "79 Validation accuracy 0.1126\n",
      "80 Validation accuracy 0.1126\n",
      "81 Validation accuracy 0.1126\n",
      "82 Validation accuracy 0.1126\n",
      "83 Validation accuracy 0.1126\n",
      "84 Validation accuracy 0.1126\n",
      "85 Validation accuracy 0.1126\n",
      "86 Validation accuracy 0.1126\n",
      "87 Validation accuracy 0.1126\n",
      "88 Validation accuracy 0.1126\n",
      "89 Validation accuracy 0.1126\n",
      "90 Validation accuracy 0.1126\n",
      "91 Validation accuracy 0.1126\n",
      "92 Validation accuracy 0.1126\n",
      "93 Validation accuracy 0.1126\n",
      "94 Validation accuracy 0.1128\n",
      "95 Validation accuracy 0.1126\n",
      "96 Validation accuracy 0.1126\n",
      "97 Validation accuracy 0.1126\n",
      "98 Validation accuracy 0.1126\n",
      "99 Validation accuracy 0.1126\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "n_batches = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cathe = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cathe_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cathe, n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})\n",
    "            \n",
    "        acc_val = accuracy.eval(feed_dict={hidden2: h2_cathe_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       \n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential scheduling\n",
    "\n",
    "$\\eta(t) = \\eta_010^{-t/r}$\n",
    "\n",
    "where:\n",
    "+ $t$ is the iteration number;\n",
    "+ $\\eta_0=0.1$, decay_rate;\n",
    "+ $r=10000$, decay_steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.96\n",
      "1 Validation accuracy 0.9736\n",
      "2 Validation accuracy 0.977\n",
      "3 Validation accuracy 0.9802\n",
      "4 Validation accuracy 0.981\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_1$ and $l_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_1 $regularizer: $S = \\sum_{i=1}^n|y_i-f(x_i)|$ \n",
    "\n",
    "$L_2 $regularizer: $S = \\sum_{i=1}^n(y_i-f(x_i))^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.01  # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_entropy\")\n",
    "    reg_loss = tf.reduce_sum(tf.abs(W1) + tf.reduce_sum(tf.abs(W2)))\n",
    "    loss = tf.add(base_loss, scale * reg_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.36\n",
      "1 Validation accuracy: 0.0154\n",
      "2 Validation accuracy: 0.243\n",
      "3 Validation accuracy: 0.0596\n",
      "4 Validation accuracy: 0.1218\n",
      "5 Validation accuracy: 0.11\n",
      "6 Validation accuracy: 0.1168\n",
      "7 Validation accuracy: 0.1126\n",
      "8 Validation accuracy: 0.1126\n",
      "9 Validation accuracy: 0.1126\n",
      "10 Validation accuracy: 0.1126\n",
      "11 Validation accuracy: 0.1126\n",
      "12 Validation accuracy: 0.1126\n",
      "13 Validation accuracy: 0.1126\n",
      "14 Validation accuracy: 0.1126\n",
      "15 Validation accuracy: 0.1126\n",
      "16 Validation accuracy: 0.1126\n",
      "17 Validation accuracy: 0.1126\n",
      "18 Validation accuracy: 0.1126\n",
      "19 Validation accuracy: 0.1126\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "keep_prob = 0.5 \n",
    "X_drop = tf.layers.dropout(X, keep_prob, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, keep_prob, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, \n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, keep_prob, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.879\n",
      "1 Validation accuracy 0.9082\n",
      "2 Validation accuracy 0.9234\n",
      "3 Validation accuracy 0.934\n",
      "4 Validation accuracy 0.9396\n",
      "5 Validation accuracy 0.9442\n",
      "6 Validation accuracy 0.9462\n",
      "7 Validation accuracy 0.9538\n",
      "8 Validation accuracy 0.953\n",
      "9 Validation accuracy 0.958\n",
      "10 Validation accuracy 0.9598\n",
      "11 Validation accuracy 0.9628\n",
      "12 Validation accuracy 0.9596\n",
      "13 Validation accuracy 0.9636\n",
      "14 Validation accuracy 0.9644\n",
      "15 Validation accuracy 0.964\n",
      "16 Validation accuracy 0.9658\n",
      "17 Validation accuracy 0.9662\n",
      "18 Validation accuracy 0.9664\n",
      "19 Validation accuracy 0.9678\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.\n",
    "weights1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights1 = tf.clip_by_norm(weights1, clip_norm=threshold, axes=1)\n",
    "clip_weights1 = tf.assign(weights1, clipped_weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.9244\n",
      "1 Validation accuracy 0.9456\n",
      "2 Validation accuracy 0.9528\n",
      "3 Validation accuracy 0.9628\n",
      "4 Validation accuracy 0.9656\n",
      "5 Validation accuracy 0.9682\n",
      "6 Validation accuracy 0.9718\n",
      "7 Validation accuracy 0.9734\n",
      "8 Validation accuracy 0.974\n",
      "9 Validation accuracy 0.9766\n",
      "10 Validation accuracy 0.9778\n",
      "11 Validation accuracy 0.9764\n",
      "12 Validation accuracy 0.9782\n",
      "13 Validation accuracy 0.9778\n",
      "14 Validation accuracy 0.979\n",
      "15 Validation accuracy 0.9776\n",
      "16 Validation accuracy 0.9794\n",
      "17 Validation accuracy 0.98\n",
      "18 Validation accuracy 0.9808\n",
      "19 Validation accuracy 0.9804\n"
     ]
    }
   ],
   "source": [
    "init= tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights1.eval()\n",
    "            clip_weights2.eval()\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                        collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                             kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                             kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9238\n",
      "1 Validation accuracy: 0.9408\n",
      "2 Validation accuracy: 0.9504\n",
      "3 Validation accuracy: 0.9588\n",
      "4 Validation accuracy: 0.9632\n",
      "5 Validation accuracy: 0.9662\n",
      "6 Validation accuracy: 0.9712\n",
      "7 Validation accuracy: 0.9724\n",
      "8 Validation accuracy: 0.974\n",
      "9 Validation accuracy: 0.9746\n",
      "10 Validation accuracy: 0.9756\n",
      "11 Validation accuracy: 0.9738\n",
      "12 Validation accuracy: 0.9772\n",
      "13 Validation accuracy: 0.9782\n",
      "14 Validation accuracy: 0.978\n",
      "15 Validation accuracy: 0.9792\n",
      "16 Validation accuracy: 0.979\n",
      "17 Validation accuracy: 0.9794\n",
      "18 Validation accuracy: 0.979\n",
      "19 Validation accuracy: 0.9776\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "       initializer=he_init, training=training):\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        for i in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons,\n",
    "                                    kernel_initializer=initializer, \n",
    "                                     name=\"hidden{}\".format(i+1))\n",
    "            bn = tf.layers.batch_normalization(inputs, training=training,\n",
    "                                                   momentum=0.9)\n",
    "            bn_drop = tf.layers.dropout(bn, 0.5, training=training)\n",
    "            inputs = tf.nn.elu(bn_drop)\n",
    "        return inputs\n",
    "\n",
    "hidden = dnn(X)\n",
    "logits = tf.layers.dense(hidden, n_outputs, name=\"outputs\")\n",
    "y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                           logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"eval\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss, name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.100951\tBest loss: 0.100951\tAccuracy: 97.58%\n",
      "1\tValidation loss: 0.115543\tBest loss: 0.100951\tAccuracy: 96.91%\n",
      "2\tValidation loss: 0.078686\tBest loss: 0.078686\tAccuracy: 98.40%\n",
      "3\tValidation loss: 0.079974\tBest loss: 0.078686\tAccuracy: 98.48%\n",
      "4\tValidation loss: 0.070548\tBest loss: 0.070548\tAccuracy: 98.28%\n",
      "5\tValidation loss: 0.085883\tBest loss: 0.070548\tAccuracy: 98.51%\n",
      "6\tValidation loss: 0.076955\tBest loss: 0.070548\tAccuracy: 98.36%\n",
      "7\tValidation loss: 0.115555\tBest loss: 0.070548\tAccuracy: 96.72%\n",
      "8\tValidation loss: 0.065526\tBest loss: 0.065526\tAccuracy: 98.55%\n",
      "9\tValidation loss: 0.055824\tBest loss: 0.055824\tAccuracy: 98.67%\n",
      "10\tValidation loss: 0.065460\tBest loss: 0.055824\tAccuracy: 98.44%\n",
      "11\tValidation loss: 0.204806\tBest loss: 0.055824\tAccuracy: 97.97%\n",
      "12\tValidation loss: 0.047918\tBest loss: 0.047918\tAccuracy: 98.87%\n",
      "13\tValidation loss: 0.075924\tBest loss: 0.047918\tAccuracy: 98.20%\n",
      "14\tValidation loss: 0.074461\tBest loss: 0.047918\tAccuracy: 98.51%\n",
      "15\tValidation loss: 0.075901\tBest loss: 0.047918\tAccuracy: 98.51%\n",
      "16\tValidation loss: 0.104291\tBest loss: 0.047918\tAccuracy: 98.12%\n",
      "17\tValidation loss: 0.051878\tBest loss: 0.047918\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.062455\tBest loss: 0.047918\tAccuracy: 98.51%\n",
      "19\tValidation loss: 27.606844\tBest loss: 0.047918\tAccuracy: 92.92%\n",
      "20\tValidation loss: 0.125308\tBest loss: 0.047918\tAccuracy: 98.08%\n",
      "21\tValidation loss: 0.124832\tBest loss: 0.047918\tAccuracy: 97.69%\n",
      "22\tValidation loss: 0.251095\tBest loss: 0.047918\tAccuracy: 98.51%\n",
      "23\tValidation loss: 0.079282\tBest loss: 0.047918\tAccuracy: 98.55%\n",
      "24\tValidation loss: 0.097505\tBest loss: 0.047918\tAccuracy: 98.55%\n",
      "25\tValidation loss: 0.593235\tBest loss: 0.047918\tAccuracy: 98.51%\n",
      "26\tValidation loss: 0.147000\tBest loss: 0.047918\tAccuracy: 97.34%\n",
      "27\tValidation loss: 0.093971\tBest loss: 0.047918\tAccuracy: 98.01%\n",
      "28\tValidation loss: 0.099402\tBest loss: 0.047918\tAccuracy: 97.85%\n",
      "29\tValidation loss: 0.295814\tBest loss: 0.047918\tAccuracy: 97.81%\n",
      "30\tValidation loss: 0.302603\tBest loss: 0.047918\tAccuracy: 98.16%\n",
      "31\tValidation loss: 1.091825\tBest loss: 0.047918\tAccuracy: 98.87%\n",
      "32\tValidation loss: 0.712257\tBest loss: 0.047918\tAccuracy: 96.72%\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "best_loss = np.infty\n",
    "checks_without_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for rnd_idx in np.array_split(np.random.permutation(len(X_train1)),\n",
    "                                     len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_idx], y_train1[rnd_idx]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./exercises/11/my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_log = tf.summary.FileWriter(\"./tf_logs/11/\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
